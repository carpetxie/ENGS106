{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment 2 - Part B: k-Nearest Neighbor Classification\n",
    "Please refer to the `README.pdf` for full laboratory instructions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "In this part, you will implement the k-Nearest Neighbor (k-NN) classifier and evaluate it on two datasets:\n",
    "- **Lenses Dataset**: A small dataset for contact lens prescription\n",
    "- **Credit Approval (CA) Dataset**: Credit card application data with binary labels (+/-)\n",
    "\n",
    "### Your Tasks\n",
    "1. **Preprocess the data**: Handle missing values and normalize features\n",
    "2. **Implement k-NN** with L2 distance\n",
    "3. **Evaluate** on both datasets for different values of k\n",
    "4. **Discuss** your results\n",
    "\n",
    "### Datasets\n",
    "The data files are located in the `credit 2017/` folder:\n",
    "- `lenses.training`, `lenses.testing`\n",
    "- `crx.data.training`, `crx.data.testing`\n",
    "- `crx.names` (describes the features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library declarations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenses - Train: (18, 3), Test: (6, 3)\n"
     ]
    }
   ],
   "source": [
    "# Data paths\n",
    "DATA_PATH = \"credit 2017/\"\n",
    "\n",
    "# Load Lenses data\n",
    "def load_lenses_data():\n",
    "    \"\"\"Load the lenses dataset.\"\"\"\n",
    "    train_data = np.loadtxt(DATA_PATH + \"lenses.training\", delimiter=',')\n",
    "    test_data = np.loadtxt(DATA_PATH + \"lenses.testing\", delimiter=',')\n",
    "    \n",
    "    # First column is ID, last column is label\n",
    "    # Syntax description: take all rows and include all columns except for first/last column\n",
    "    X_train = train_data[:, 1:-1]\n",
    "    y_train = train_data[:, -1]\n",
    "    X_test = test_data[:, 1:-1]\n",
    "    y_test = test_data[:, -1]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Load Credit Approval data\n",
    "def load_credit_data():\n",
    "    \"\"\"\n",
    "    Load the Credit Approval dataset.\n",
    "    Note: This dataset contains missing values (?) and mixed types.\n",
    "    You will need to preprocess it.\n",
    "    \"\"\"\n",
    "    train_data = np.loadtxt(DATA_PATH + \"crx.data.training\", delimiter=',')\n",
    "    test_data = np.loadtxt(DATA_PATH + \"crx.data.testing\", delimiter=',')\n",
    "\n",
    "    X_train = train_data[:, :-1]\n",
    "    y_train = train_data[:, -1]\n",
    "\n",
    "    X_test = test_data[:, 1:-1]\n",
    "    y_test = test_data[:, -1]\n",
    "    # TODO: Implement data loading\n",
    "    # The data is comma-separated\n",
    "    # Missing values are marked with '?'\n",
    "    # Last column is the label ('+' or '-')\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Test loading lenses data\n",
    "X_train_lenses, y_train_lenses, X_test_lenses, y_test_lenses = load_lenses_data()\n",
    "print(f\"Lenses - Train: {X_train_lenses.shape}, Test: {X_test_lenses.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Data Preprocessing\n",
    "For the Credit Approval dataset, you need to:\n",
    "1. **Handle missing values** (marked with '?'):\n",
    "   - Categorical features: replace with mode/median\n",
    "   - Numerical features: replace with label-conditioned mean\n",
    "2. **Normalize features** using z-scaling:\n",
    "   $$z_i^{(m)} = \\frac{x_i^{(m)} - \\mu_i}{\\sigma_i}$$\n",
    "\n",
    "Document exactly how you handle each feature!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_credit_data(train_file, test_file):\n",
    "    \"\"\"\n",
    "    Preprocess the Credit Approval dataset.\n",
    "    \n",
    "    Steps:\n",
    "    1. Load the data\n",
    "    2. Handle missing values\n",
    "    3. Encode categorical variables\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train, y_train, X_test, y_test : numpy arrays\n",
    "    \"\"\"\n",
    "    # Load using pandas (only for loading)\n",
    "    import pandas as pd\n",
    "    X_train = pd.read_csv(train_file, header=None, na_values='?').values\n",
    "    X_test = pd.read_csv(test_file, header=None, na_values='?').values\n",
    "    \n",
    "    # Separate features and labels\n",
    "    y_train = X_train[:, -1]\n",
    "    y_test = X_test[:, -1]\n",
    "    X_train = X_train[:, :-1]\n",
    "    X_test = X_test[:, :-1]\n",
    "    \n",
    "    # Feature indices: categorical vs numerical\n",
    "    categorical_features = [0, 3, 4, 5, 6, 8, 9, 11, 12]\n",
    "    numerical_features = [1, 2, 7, 10, 13, 14]\n",
    "    \n",
    "    # Handle missing values for categorical features (use mode)\n",
    "    for col in categorical_features:\n",
    "        # Find NaN mask\n",
    "        mask = np.array([isinstance(x, float) and np.isnan(x) for x in X_train[:, col]])\n",
    "        \n",
    "        if mask.any():\n",
    "            # Get mode using Counter\n",
    "            non_missing = X_train[~mask, col]\n",
    "            mode_val = Counter(non_missing).most_common(1)[0][0]\n",
    "            \n",
    "            X_train[mask, col] = mode_val\n",
    "            test_mask = np.array([isinstance(x, float) and np.isnan(x) for x in X_test[:, col]])\n",
    "            X_test[test_mask, col] = mode_val\n",
    "    \n",
    "    # Handle missing values for numerical features (use label-conditioned mean)\n",
    "    for col in numerical_features:\n",
    "        for label in ['+', '-']:\n",
    "            # Find missing values for this label\n",
    "            label_mask_train = y_train == label\n",
    "            missing_mask_train = np.array([isinstance(x, float) and np.isnan(x) for x in X_train[:, col]])\n",
    "            train_mask = label_mask_train & missing_mask_train\n",
    "            \n",
    "            label_mask_test = y_test == label\n",
    "            missing_mask_test = np.array([isinstance(x, float) and np.isnan(x) for x in X_test[:, col]])\n",
    "            test_mask = label_mask_test & missing_mask_test\n",
    "            \n",
    "            # Compute mean for this label\n",
    "            label_data = X_train[label_mask_train, col]\n",
    "            label_data_float = np.array([float(x) for x in label_data if not (isinstance(x, float) and np.isnan(x))])\n",
    "            label_mean = np.mean(label_data_float)\n",
    "            \n",
    "            X_train[train_mask, col] = label_mean\n",
    "            X_test[test_mask, col] = label_mean\n",
    "    \n",
    "    # Convert numerical features to float (we gotta keep categorical as strings tho)\n",
    "    for col in numerical_features:\n",
    "        X_train[:, col] = X_train[:, col].astype(float)\n",
    "        X_test[:, col] = X_test[:, col].astype(float)\n",
    "    \n",
    "    # Encode labels: '+' -> 1, '-' -> 0\n",
    "    y_train = np.array([1 if label == '+' else 0 for label in y_train])\n",
    "    y_test = np.array([1 if label == '+' else 0 for label in y_test])\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def z_normalize(X_train, X_test, feature_indices):\n",
    "    \"\"\"\n",
    "    Apply z-score normalization to specified features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_test : numpy arrays\n",
    "    feature_indices : list of indices for numerical features\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train_normalized, X_test_normalized : numpy arrays\n",
    "    \"\"\"\n",
    "    X_train_normalized = X_train.copy()\n",
    "    X_test_normalized = X_test.copy()\n",
    "    \n",
    "    for col in feature_indices:\n",
    "        mean = np.mean(X_train[:, col])\n",
    "        std = np.std(X_train[:, col])\n",
    "        \n",
    "        if std > 0:\n",
    "            X_train_normalized[:, col] = (X_train[:, col] - mean) / std\n",
    "            X_test_normalized[:, col] = (X_test[:, col] - mean) / std\n",
    "    \n",
    "    return X_train_normalized, X_test_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement k-NN Classifier\n",
    "Implement k-NN with L2 (Euclidean) distance:\n",
    "$$\\mathcal{D}_{L2}(\\mathbf{a}, \\mathbf{b}) = \\sqrt{\\sum_i (a_i - b_i)^2}$$\n",
    "\n",
    "For **categorical attributes**, use:\n",
    "- Distance = 1 if values are different\n",
    "- Distance = 0 if values are the same\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_distance(a, b):\n",
    "    \"\"\"\n",
    "    Compute L2 (Euclidean) distance between two vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    a, b : numpy arrays of same shape\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    distance : float\n",
    "    \"\"\"\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "\n",
    "    squared_distance = 0.0\n",
    "    for i in range(len(a)):\n",
    "        # Check if categorical (string) or numerical\n",
    "        if isinstance(a[i], str) or isinstance(b[i], str):\n",
    "            # Categorical: 1 if different, 0 if same\n",
    "            squared_distance += 1.0 if a[i] != b[i] else 0.0\n",
    "        else:\n",
    "            # Numerical\n",
    "            squared_distance += (a[i] - b[i]) ** 2\n",
    "    \n",
    "    return np.sqrt(squared_distance)\n",
    "\n",
    "\n",
    "\n",
    "def knn_predict(X_train, y_train, X_test, k):\n",
    "    \"\"\"\n",
    "    Predict labels for test data using k-NN.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : numpy array of shape (n_train, n_features)\n",
    "    y_train : numpy array of shape (n_train,)\n",
    "    X_test : numpy array of shape (n_test, n_features)\n",
    "    k : int, number of neighbors\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions : numpy array of shape (n_test,)\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = []\n",
    "    \n",
    "    for row in X_test:                                                                                            \n",
    "      distances = np.array([l2_distance(sample, row) for sample in X_train])\n",
    "      k_nearest_indices = np.argsort(distances)[:k]  # k smallest distances\n",
    "      k_nearest_labels = y_train[k_nearest_indices]\n",
    "\n",
    "      # We use counter -> taking the most common label -> the first one -> label\n",
    "      prediction = Counter(k_nearest_labels).most_common(1)[0][0]\n",
    "      predictions.append(prediction)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    return predictions\n",
    "        \n",
    "    \n",
    "    # TODO: Implement k-NN prediction\n",
    "    # For each test sample:\n",
    "    #   1. Compute distance to all training samples\n",
    "    #   2. Find k nearest neighbors\n",
    "    #   3. Predict using majority voting\n",
    "\n",
    "\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute classification accuracy.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    accuracy : float (between 0 and 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # I think I know how to use ternary functions fluently now :) \n",
    "    return np.sum(np.array([y_true[i] == y_pred[i] for i in range(len(y_true))])) / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Evaluate on Lenses Dataset\n",
    "Test your k-NN implementation on the Lenses dataset for different values of k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Evaluate k-NN on Lenses dataset\n",
    "# Try different values of k (e.g., 1, 3, 5, 7)\n",
    "\n",
    "k_values = [1, 3, 5, 7]\n",
    "lenses_results = []\n",
    "# \n",
    "for k in k_values:\n",
    "     predictions = knn_predict(X_train_lenses, y_train_lenses, X_test_lenses, k)\n",
    "     accuracy = compute_accuracy(y_test_lenses, predictions)\n",
    "     lenses_results.append((k, accuracy))\n",
    "     print(f\"k={k}: Accuracy = {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Evaluate on Credit Approval Dataset\n",
    "First preprocess the data, then evaluate k-NN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Preprocess Credit Approval data\n",
    "X_train_credit, y_train_credit, X_test_credit, y_test_credit = preprocess_credit_data(\n",
    "     DATA_PATH + \"crx.data.training\",\n",
    "     DATA_PATH + \"crx.data.testing\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1: Accuracy = 0.6522\n",
      "k=2: Accuracy = 0.6522\n",
      "k=3: Accuracy = 0.6812\n",
      "k=4: Accuracy = 0.6739\n",
      "k=5: Accuracy = 0.6667\n",
      "k=6: Accuracy = 0.6812\n",
      "k=7: Accuracy = 0.6304\n",
      "k=8: Accuracy = 0.6377\n"
     ]
    }
   ],
   "source": [
    "# TODO: Evaluate k-NN on Credit Approval dataset\n",
    "credit_results = []\n",
    " \n",
    "for k in range(1, 9):\n",
    "     predictions = knn_predict(X_train_credit, y_train_credit, X_test_credit, k)\n",
    "     accuracy = compute_accuracy(y_test_credit, predictions)\n",
    "     credit_results.append((k, accuracy))\n",
    "     print(f\"k={k}: Accuracy = {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Discussion\n",
    "\n",
    "### Results Table\n",
    "\n",
    "| Dataset | k=1 | k=3 | k=5 | k=7 |\n",
    "|---------|-----|-----|-----|-----|\n",
    "| Lenses | 1 | 1 | 0.5 | 0.83 |\n",
    "| Credit Approval | 0.6522 | 0.6812 | 0.6667 | 0.66304 |\n",
    "\n",
    "### Discussion\n",
    "*Answer these questions:*\n",
    "1. **Which value of k works best for each dataset? Why do you think that is?**\n",
    "For lenses k=1 and for credit approval for k = 3 worked the best. We have limited test samples particularly for lenses. \n",
    "\n",
    "As for credit approval I am not too sure. This is quite disappointing. Perhaps the transition from categorical to numerical \n",
    "was not a perfect translation. I would say perhaps that the optimal k likely scales with larger datasets. \n",
    "\n",
    "2. **How did preprocessing affect your results on the Credit Approval dataset?**\n",
    "Well the credit approval dataset is more full without NaN values. It was also missing  value imputation preserved class-specific patterns (label-conditioned mean for numerical features).\n",
    "\n",
    "Without preprocessing, the dataset would be unusable (can't compute distances on strings/NaN). So preprocessing didn't artificially inflate accuracy, it just made the data usable while preserving its structure.\n",
    "3. **What are the trade-offs of using different values of k?**\n",
    "The bias variance tradeoff. If we have low k, there is low bias and higher variance as the test point\n",
    "can overfit to points incredibly close to it. Vice versa. As k increases, we begin to underfit as the choise of the test\n",
    "label will begin to approach the proportion split of the data. \n",
    "\n",
    "4. **What did you learn from this exercise?**\n",
    "I learned particularly how to become comfortable with low level understanding of manipulating and transforming arrays. \n",
    "Using ternary operators really forces you to visualize since you aren't breaking it down step by step in distinct for loops. \n",
    "\n",
    "In the age of AI where perhaps my work will be replaced by AI, I begin to wonder the importance of low level coding. The utility comes not from the code I write, but the understanding I gain from writing code. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
